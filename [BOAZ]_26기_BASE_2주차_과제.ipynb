{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msLr-G13YUo"
      },
      "source": [
        "# RNN Sample Code in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JrL9Fl93d0T"
      },
      "source": [
        "- 직접 읽어보며 돌려볼 수 있는 **쉬운** 예제 코드~\n",
        "- 코드 간단 설명:\n",
        "  - 길이 12짜리 binary sequence(0/1)를 입력으로 받아서, 시퀀스 안에 1-0-1 pattern이 한 번이라도 등장하면 1, 아니면 0을 맞추는 binary classification을 수행하는 RNN 분류기\n",
        "  - 마지막에는 demo sequence로 예측 + hidden state 변화까지 출력하는 프로그램"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EO04CkcR4bFR"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLXMSKiQ9k6l"
      },
      "source": [
        "- 정답 label 만드는 함수\n",
        "- 역할:\n",
        "  - sequence(e.g., [1,0,1,0,0,...]) 안에 연속된 3칸이 1-0-1인 구간이 있는지 검사\n",
        "  - 있으면 label=1 / 없으면 label=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rfrge_Gq8ZFx"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzaGLdAbGUSr"
      },
      "source": [
        "- 학습용 data 만드는 PatternDataset 클래스\n",
        "- 깨알 상식) PyTorch에서 Dataset은 \"데이터를 꺼내는 방식\"을 표준화한 클래스~\n",
        "  - 이걸 상속받아서 내가 하고자 하는 task에 부합하는 나만의 커스텀 Dataset 클래스를 만들어서 모델에 먹이는 겁니다 얍얍"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4legZWrO8YJp"
      },
      "outputs": [],
      "source": [
        "# Dataset이 하는 일은 \"모델에 넣기 좋은 형태\"로 데이터를 제공하는 것!\n",
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        # (입력 시퀀스, 정답 label)을 n_samples개만큼 저장\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]  # 1) seq_len 길이의 random sequence 생성(0/1)\n",
        "            label = has_101_pattern(seq)                          # 2) has_101_pattern(seq)로 정답 label 생성\n",
        "            self.data.append((seq, label))                        # 3) (seq, label)을 self.data에 저장\n",
        "\n",
        "    # Dataset 안에 sample이 몇 개인지 알려주는 매직 메소드!\n",
        "    # 이걸로 보통 DataLoader가 \"전체 크기\"를 알 수 있게 합니다\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # idx번째 데이터를 꺼내서 pytorch tensor로 변환해주는 매직 메소드\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)         # (T,) = (12,) / embedding은 정수 인덱스를 받기 때문에 dtype으로 long을 사용합니다!\n",
        "        y = torch.tensor(label, dtype=torch.float32)    # scalar / BCEWithLogitsLoss가 float label(0.0/1.0)을 기대하는 편이라 dtype으로 float32를 사용했어요\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTROYjWUINLU"
      },
      "source": [
        "- RNN 모델 클래스\n",
        "- pytorch에서 모델 클래스는 일반적으로 nn.Module을 상속해서 만들어요~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9B39dpKJ7Kl_"
      },
      "outputs": [],
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # 학습 연산을 위해 (0/1) -> '벡터'로 변환\n",
        "        self.rnn = nn.RNN(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True) # sequence를 왼쪽부터 읽으면서 hidden state를 update\n",
        "        self.fc = nn.Linear(hidden_dim, 1) # 마지막 hidden state로 이진 분류 점수(logit) 출력\n",
        "\n",
        "    # 일반 학습/평가용 feedforward 순전파 함수\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        NOTE: B는 batch size, T는 시퀀스의 길이!\n",
        "\n",
        "        input x: (B, T) 0/1 token\n",
        "        return: logits (B,)\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (B, T, E) / 벡터화\n",
        "        out, h_n = self.rnn(emb)       # 각 시점의 hidden 기록인 out: (B, T, H) / 마지막 hidden state인 h_n: (1, B, H)\n",
        "        last_h = h_n[-1]               # (B, H) / 마지막 hidden만 추출\n",
        "        logits = self.fc(last_h)       # (B, H) -> (B, 1)\n",
        "        return logits.squeeze(1)       # (B,) / loss 계산 편하게 하기 위해 주로 이렇게 squeeze()라는 함수를 사용하여 모양을 맞춰줍니다\n",
        "\n",
        "    def forward_with_trace(self, x):\n",
        "        \"\"\"\n",
        "        시각화를 통해 이해할 수 있도록 time step별 hidden(out)과 마지막 예측(logits)을 함께 리턴하는 함수\n",
        "        x: (1, T) 단일 시퀀스만 넣는 것을 권장함\n",
        "        \"\"\"\n",
        "        emb = self.embed(x)            # (1, T, E)\n",
        "        out, h_n = self.rnn(emb)       # out: (1, T, H)\n",
        "        last_h = h_n[-1]               # (1, H)\n",
        "        logits = self.fc(last_h)       # (1, 1)\n",
        "        return logits.squeeze(1), out.squeeze(0)  # logits: (1,), out: (T, H)\n",
        "        # out.squeeze(0) 추가로 한 이유 : batch=1을 넣으면 shape이 (1,T,H)인데 이 batch의 차원(1)을 제거하여 (T,H)로 보기 좋게 만든것!\n",
        "        # (크게 중요한 건 아닌데 그냥 궁금하실까봐,,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXV7srCf9Jhm"
      },
      "source": [
        "- 메인 함수: train()\n",
        "- 내부 로직 STEP BY STEP 설명:\n",
        "  1. (train/val) dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model / loss function / optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증(val) 정확도 출력\n",
        "  6. 마지막에 demo sequence 1개 넣어서 확률 출력\n",
        "  7. demo sequence에서 시간별 hidden state 일부 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAHlsjUj8Ntd",
        "outputId": "9583e647-39a4-4f29-b97a-30486b1d2c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5795  val_acc=0.7650\n",
            "[Epoch 2] train_loss=0.5674  val_acc=0.7650\n",
            "[Epoch 3] train_loss=0.5368  val_acc=0.7830\n",
            "[Epoch 4] train_loss=0.4833  val_acc=0.8170\n",
            "[Epoch 5] train_loss=0.3360  val_acc=0.9820\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9355\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.006, 0.375, -0.834, 0.199, -0.562, 0.113]\n",
            "t= 1, x_t=0 -> h_t[:6]=[0.17, -0.641, -0.02, 0.134, 0.842, 0.288]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.504, 0.125, -0.753, -0.339, 0.257, -0.583]\n",
            "t= 9, x_t=0 -> h_t[:6]=[-0.824, -0.93, -0.328, -0.865, 0.959, -0.792]\n",
            "t=10, x_t=0 -> h_t[:6]=[-0.834, -0.932, -0.331, -0.87, 0.959, -0.798]\n",
            "t=11, x_t=0 -> h_t[:6]=[-0.84, -0.934, -0.333, -0.874, 0.959, -0.8]\n"
          ]
        }
      ],
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    \"\"\"\n",
        "    <헷갈리는 개념 (코드 지피티 딸깍하지 말고 이젠 꼭 알아두자)>\n",
        "    - Dataset: 데이터 1개를 어떻게 꺼낼지 정의\n",
        "    - DataLoader: 여러 개를 묶어서 batch를 만들고, 섞고, 반복 가능한 형태로 제공\n",
        "    - shuffle : train은 섞어서 학습이 안정적이고 (True) / val은 평가하는 거니까 섞을 필요 없음 (False)\n",
        "    \"\"\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    # model/loss function/optimizer 준비\n",
        "    model = SimpleRNNClassifier(embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Epoch train loop\n",
        "    for epoch in range(1, 6):\n",
        "        model.train()                                   # dropout/batch regularization 등의 mode들이 train 모드로 바뀜\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:                       # batch 단위로 (x, y) 받음\n",
        "            x, y = x.to(device), y.to(device)           # x:(B,T), y:(B,)\n",
        "\n",
        "            logits = model(x)                           # (B,) / forward 실행\n",
        "            loss = criterion(logits, y)                 # scalar값 (정답 y와 예측 점수인 logit을 비교하여 loss값 계산)\n",
        "\n",
        "            optimizer.zero_grad()                       # 이전 gradient를 0으로\n",
        "            loss.backward()                             # backpropagation\n",
        "            optimizer.step()                            # parameter update\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        train_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "        # validation\n",
        "        model.eval()                                    # 평가 모드\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():                           # 평가이므로 학습 때와 달리 gradient 계산 하지 않음\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                preds = (probs >= 0.5).float()          # 0.5 이상이면 1로 예측하도록 구현\n",
        "                correct += (preds == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # DEMO: hidden state 흐름을 출력해보자!\n",
        "    # ----------------------------\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,0,0,0,0,0,0]   # 패턴 101이 있는 입력 시퀀스\n",
        "    demo = torch.tensor([demo_seq], dtype=torch.long).to(device)  # (1,T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logit, h_trace = model.forward_with_trace(demo)\n",
        "        prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(\"Final prob(pattern=101):\", round(prob, 4))\n",
        "\n",
        "    # hidden trace 일부 출력(앞 3스텝 + 마지막 3스텝)\n",
        "    h_cpu = h_trace.cpu()  # (T,H)\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    for t in list(range(3)) + list(range(len(demo_seq)-3, len(demo_seq))):\n",
        "        vec = h_cpu[t][:6].tolist()  # hidden_dim 16 중 앞 6개만 보기\n",
        "        vec = [round(v, 3) for v in vec]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={vec}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkUeto3kjsbW"
      },
      "source": [
        "- 위 코드에서 demo_seq 변수를 아래 두 가지로 바꿔서 각각 실행해보세요~\n",
        "  - 패턴 있음: [1,0,1,0,0,0,0,0,0,0,0,0] → 확률 높아야 함\n",
        "  - 패턴 없음: [1,1,0,0,1,1,0,0,1,1,0,0] → 확률 낮아야 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBSysnq3kNjm"
      },
      "source": [
        "### Q0. 위 코드의 출력 결과 분석 & 두 가지 입력을 넣었을 때 각각의 결과를 비교 분석하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pifQ80I8kZr_"
      },
      "source": [
        "Ans)\n",
        "\n",
        "[1,0,1,0,0,0,0,0,0,0,0,0]는 해당 패턴이 존재하므로 0.9584로 확률이 높게 나오며\n",
        "\n",
        "[1,1,0,0,1,1,0,0,1,1,0,0]는 해당 패턴이 존재하지 않으므로 0.2191로 확률이 낮게 출력됨\n",
        "\n",
        "이를 통해 RNN이 hidden state에 관해 패턴 존재의 여부를 메모리처럼 저장하고 있다는 것을 알 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njt9mJdlkiDQ"
      },
      "source": [
        "# LSTM Sample code in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lrNsxJFRGy"
      },
      "source": [
        "- 아래는 위와 동일하게 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kW-QeXmXkhF9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XMoSaivQFF7Q"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uo-gXIZOFF5Q"
      },
      "outputs": [],
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hzsnnU5FL2k"
      },
      "source": [
        "- 여기서부터 LSTM 모델 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FyAeyB5sFF2b"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)  # (0/1) -> 벡터로 변환\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                         # (batch, seq_len, embed_dim)\n",
        "        out, (h_n, c_n) = self.lstm(emb)            # out: (batch, seq_len, hidden_dim)\n",
        "                                                    # h_n: (num_layers, batch, hidden_dim)\n",
        "                                                    # c_n: (num_layers, batch, hidden_dim)\n",
        "\n",
        "        last_h = h_n[-1]                            # (batch, hidden_dim)  마지막 layer의 마지막 hidden\n",
        "        logit = self.fc(last_h)                     # (batch, 1)\n",
        "        return logit, out, (h_n, c_n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAsdRO6FFUe-"
      },
      "source": [
        "RNN과의 차이점??\n",
        "\n",
        "1. nn.RNN -> nn.LSTM\n",
        "2. lSTM은 hidden state(h) 말고도 cell state(c)가 추가되었다는 점\n",
        "3. forward 결과에 따른 형태? (output, (h_n, c_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5EtTq-8FVOX"
      },
      "source": [
        "- 메인 함수 : train()\n",
        "- 내부 로직 step by step 설명:\n",
        "  1. train, val dataset 생성\n",
        "  2. DataLoader로 배치 묶기\n",
        "  3. model, loss function, optimizer 준비\n",
        "  4. epoch 반복하며 train\n",
        "  5. epoch마다 검증 정확도 출력\n",
        "  6. 마지막에 demo seq 하나 넣어서 확률 출력\n",
        "  7. demo seq에서 시간별 hidden state 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "stLhRCNMFF0L"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleLSTMClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()  # logit을 바로 넣는 BCE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)               # x:(B,12), y:(B,1)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)                           # logit:(B,1)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)                  # (B,1)\n",
        "                pred = (prob >= 0.5).float()                 # (B,1)\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch:02d}] loss={avg_loss:.4f} | val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0] # 패턴 없음 -> 확률 낮아야 함\n",
        "\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "    logit, out_all, (h_n, c_n) = model(x_demo)\n",
        "\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "    print(\"\\n--- DEMO ---\")\n",
        "    print(\"demo_seq:\", demo_seq)\n",
        "    print(f\"pred_prob(pattern=1): {prob:.4f}\")\n",
        "\n",
        "    # 시간별 hidden state 일부 출력\n",
        "    # out_all: (1, seq_len, hidden_dim)  -> time step별 hidden이 들어있음 (LSTM의 output)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (seq_len, hidden_dim)\n",
        "\n",
        "    print(\"\\n[time step별 hidden state 앞 6개 차원만 출력]\")\n",
        "    for t in range(out_all.size(0)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        print(f\"t={t:02d}, x={demo_seq[t]} -> h_t[:6]={h_t}\")\n",
        "\n",
        "    # (참고) 마지막 hidden/cell state도 같이 보기\n",
        "    last_h = h_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    last_c = c_n[-1].squeeze(0).detach().cpu()    # (hidden_dim,)\n",
        "    print(\"\\n[마지막 state 요약]\")\n",
        "    print(\"last_h[:6] =\", last_h[:6].numpy())\n",
        "    print(\"last_c[:6] =\", last_c[:6].numpy())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxJeYuWFcOQ"
      },
      "source": [
        "돌려돌려"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSe_IZ-DFFyD",
        "outputId": "20deb65f-7cb0-4479-f413-b6084b6ab578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] loss=0.5920 | val_acc=0.7330\n",
            "[Epoch 02] loss=0.5294 | val_acc=0.7620\n",
            "[Epoch 03] loss=0.4925 | val_acc=0.7780\n",
            "[Epoch 04] loss=0.3499 | val_acc=0.9090\n",
            "[Epoch 05] loss=0.1425 | val_acc=1.0000\n",
            "\n",
            "--- DEMO ---\n",
            "demo_seq: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "pred_prob(pattern=1): 0.9883\n",
            "\n",
            "[time step별 hidden state 앞 6개 차원만 출력]\n",
            "t=00, x=1 -> h_t[:6]=[-0.15675336 -0.12878327  0.4828112  -0.14847915  0.1871993   0.33261827]\n",
            "t=01, x=0 -> h_t[:6]=[-0.0095544   0.62405086 -0.27282107 -0.21487817 -0.56118083  0.08587229]\n",
            "t=02, x=1 -> h_t[:6]=[-0.46709535  0.4405527   0.62653965 -0.54104954 -0.64212805  0.647255  ]\n",
            "t=03, x=0 -> h_t[:6]=[-0.73235315  0.79952717 -0.04266965 -0.6827187  -0.8810551   0.7902502 ]\n",
            "t=04, x=0 -> h_t[:6]=[-0.88922834  0.71040714  0.02241221 -0.8432334  -0.9176525   0.908965  ]\n",
            "t=05, x=0 -> h_t[:6]=[-0.9406596   0.87546057 -0.02693075 -0.87774605 -0.9569214   0.954206  ]\n",
            "t=06, x=1 -> h_t[:6]=[-0.94118106  0.8793672   0.6190025  -0.9547527  -0.9803582   0.9497416 ]\n",
            "t=07, x=1 -> h_t[:6]=[-0.9294004   0.92240673  0.6120414  -0.9593889  -0.9911847   0.9453763 ]\n",
            "t=08, x=0 -> h_t[:6]=[-0.9681669   0.97500265  0.13551937 -0.91530585 -0.989422    0.9823324 ]\n",
            "t=09, x=0 -> h_t[:6]=[-0.9792401   0.9788936   0.16292489 -0.93546826 -0.9839791   0.98982227]\n",
            "t=10, x=0 -> h_t[:6]=[-0.9766755   0.9872684   0.10248178 -0.92553806 -0.98670954  0.9883077 ]\n",
            "t=11, x=0 -> h_t[:6]=[-0.9783444   0.989908    0.11097186 -0.9294116  -0.98584855  0.98945904]\n",
            "\n",
            "[마지막 state 요약]\n",
            "last_h[:6] = [-0.9783444   0.989908    0.11097186 -0.9294116  -0.98584855  0.98945904]\n",
            "last_c[:6] = [-4.825798    2.7585263   0.18700711 -4.933533   -3.5673134   4.9617095 ]\n"
          ]
        }
      ],
      "source": [
        "model = train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XZsF577Fgy1"
      },
      "source": [
        "- RNN과 비교할 점 :\n",
        "  1. val_acc\n",
        "  2. train_loss\n",
        "  3. demo prob\n",
        "\n",
        "\n",
        "- RNN vs LSTM\n",
        "\n",
        "  현재는 장기기억이 필요 없어서 유사한 상황.\n",
        "  \n",
        "  trade-off 중요성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRjTLGz8FkJx"
      },
      "source": [
        "# GRU Sample code in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FZQRzd_yFFwH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IyxVJD_HFFtX"
      },
      "outputs": [],
      "source": [
        "def has_101_pattern(seq):\n",
        "    for i in range(len(seq) - 2):\n",
        "        if seq[i] == 1 and seq[i+1] == 0 and seq[i+2] == 1:\n",
        "            return 1\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Py8J1mxyFFq3"
      },
      "outputs": [],
      "source": [
        "class PatternDataset(Dataset):\n",
        "    def __init__(self, n_samples=5000, seq_len=12):\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            seq = [random.randint(0, 1) for _ in range(seq_len)]\n",
        "            label = has_101_pattern(seq)\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)\n",
        "        y = torch.tensor([label], dtype=torch.float)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXUAnZxhFtM_"
      },
      "source": [
        "- GRU 클래스:\n",
        "  nn.GRU\n",
        "  \n",
        "  LSTM처럼 cell state(c)가 없고, hidden state(h) 하나만 유지\n",
        "\n",
        "  forward 결과로 out, h_n\n",
        "  \n",
        "  out : 모든 time step의 hidden (batch, seq_len, hidden_dim)\n",
        "\n",
        "  h_n : 마지막 hidden (num_layers, batch, hidden_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n_7qciOPFFoo"
      },
      "outputs": [],
      "source": [
        "class SimpleGRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=2, embed_dim=8, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embed(x)                 # (B, T, E)\n",
        "        out, h_n = self.gru(emb)            # out: (B, T, H), h_n: (1, B, H)\n",
        "        last_h = h_n[-1]                    # (B, H)\n",
        "        logit = self.fc(last_h)             # (B, 1)\n",
        "        return logit, out, h_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nrZXy-8iFFlr"
      },
      "outputs": [],
      "source": [
        "def train_gru():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    train_ds = PatternDataset(n_samples=6000, seq_len=12)\n",
        "    val_ds   = PatternDataset(n_samples=1000, seq_len=12)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "    model = SimpleGRUClassifier(vocab_size=2, embed_dim=8, hidden_dim=16).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    n_epochs = 5\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logit, _, _ = model(x)\n",
        "            loss = criterion(logit, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_ds)\n",
        "\n",
        "        # ---- val ----\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logit, _, _ = model(x)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                pred = (prob >= 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"[Epoch {epoch}] train_loss={avg_loss:.4f}  val_acc={acc:.4f}\")\n",
        "\n",
        "    # ---- demo ----\n",
        "    model.eval()\n",
        "\n",
        "    demo_seq = [1,0,1,0,0,0,1,1,0,0,0,0]  # 패턴 있음\n",
        "    x_demo = torch.tensor(demo_seq, dtype=torch.long).unsqueeze(0).to(device)  # (1, 12)\n",
        "\n",
        "    logit, out_all, h_n = model(x_demo)\n",
        "    prob = torch.sigmoid(logit).item()\n",
        "\n",
        "    print(\"\\n=== Demo ===\")\n",
        "    print(\"Sequence:\", demo_seq)\n",
        "    print(f\"Final prob(pattern=101): {prob:.4f}\")\n",
        "\n",
        "    # time step별 hidden state 출력 (앞 6개 차원)\n",
        "    out_all = out_all.squeeze(0).detach().cpu()  # (T, H)\n",
        "\n",
        "    print(\"\\nHidden state trace (show first 3 and last 3 time steps):\")\n",
        "    T = out_all.size(0)\n",
        "    for t in list(range(3)) + list(range(T-3, T)):\n",
        "        h_t = out_all[t, :6].numpy()\n",
        "        # 보기 좋게 소수점 3자리로\n",
        "        h_t_fmt = [float(f\"{v:.3f}\") for v in h_t]\n",
        "        print(f\"t={t:2d}, x_t={demo_seq[t]} -> h_t[:6]={h_t_fmt}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHGR8JkFFjY",
        "outputId": "03c309b5-5fd7-4b2e-8b7c-1a9fed1a0005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5792  val_acc=0.7420\n",
            "[Epoch 2] train_loss=0.5238  val_acc=0.8030\n",
            "[Epoch 3] train_loss=0.3073  val_acc=0.9860\n",
            "[Epoch 4] train_loss=0.0650  val_acc=1.0000\n",
            "[Epoch 5] train_loss=0.0245  val_acc=1.0000\n",
            "\n",
            "=== Demo ===\n",
            "Sequence: [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0]\n",
            "Final prob(pattern=101): 0.9951\n",
            "\n",
            "Hidden state trace (show first 3 and last 3 time steps):\n",
            "t= 0, x_t=1 -> h_t[:6]=[0.214, -0.562, 0.202, 0.181, 0.711, -0.229]\n",
            "t= 1, x_t=0 -> h_t[:6]=[0.226, 0.936, -0.741, 0.427, -0.613, -0.197]\n",
            "t= 2, x_t=1 -> h_t[:6]=[0.708, 0.617, -0.444, 0.831, 0.834, -0.911]\n",
            "t= 9, x_t=0 -> h_t[:6]=[0.971, 1.0, 0.363, 0.961, 0.965, -0.924]\n",
            "t=10, x_t=0 -> h_t[:6]=[0.968, 1.0, 0.382, 0.974, 0.962, -0.925]\n",
            "t=11, x_t=0 -> h_t[:6]=[0.965, 1.0, 0.41, 0.979, 0.96, -0.925]\n"
          ]
        }
      ],
      "source": [
        "gru_model = train_gru()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnW68WglF24v"
      },
      "source": [
        "- LSTM vs GRU ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmuBRGxgF5ne"
      },
      "source": [
        "# LSTM & GRU 과제"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM42wRXOF8Tu"
      },
      "source": [
        "모델의 장기기억 성능을 비교하기 위한 코드입니다.\n",
        "\n",
        "코드 중간의 빈칸을 채우면서, 매애앤 아래의 답변을 채워주시면 됩니다.\n",
        "\n",
        "모르면 인공지능을 사용해도 좋지만, sample code로도 풀 수 있으니 최대한 본인의 힘으로 해보면 좋겠습니다 !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bItDmp57FyrU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QkVj3qZuFyqA",
        "outputId": "686bfc2b-fe04-4a0e-adee-0b54f2763cf2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# tmi) 11/7은 제 생일입니다. 감사합니다.\n",
        "def set_seed(seed=117):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(117)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hZrbG1IeFyoJ"
      },
      "outputs": [],
      "source": [
        "class LongMemoryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, n_samples=8000, T=80):\n",
        "        self.T = T\n",
        "        self.data = []\n",
        "        for _ in range(n_samples):\n",
        "            first_bit = random.randint(0, 1)          # 기억해야 할 정보\n",
        "            middle = [random.randint(0, 1) for _ in range(T-2)]\n",
        "            seq = [first_bit] + middle + [2]          # 마지막은 DELIM=2\n",
        "            label = first_bit\n",
        "            self.data.append((seq, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, label = self.data[idx]\n",
        "        x = torch.tensor(seq, dtype=torch.long)               # (T,)\n",
        "        y = torch.tensor([label], dtype=torch.float)          # (1,)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLi1JgS9GFQ8"
      },
      "source": [
        "1. lstm 모델 빈칸 채우기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HWz4-GNaFymh"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어를 선언하세요.\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : LSTM 레이어를 선언하세요. (batch_first=True)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO: 임베딩을 통과시키세요.\n",
        "        emb = self.embed(x)\n",
        "        # TODO : LSTM에 넣고 out, (h_n, c_n)을 받으세요.\n",
        "        out, (h_n, c_n) = self.lstm(emb)\n",
        "        # TODO : 마지막 hidden(last_h)을 얻으세요.\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, (h_n, c_n)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dXxnYlGGih"
      },
      "source": [
        "2. gru 모델 빈칸 채우기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SXYh1847Fyko"
      },
      "outputs": [],
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size=3, embed_dim=8, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # TODO : 임베딩 레이어\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        # TODO : GRU 레이어 (batch_first=True)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        # TODO : 임베딩\n",
        "        emb = self.embed(x)\n",
        "        # TODO : GRU forward로 out, h_n 받기\n",
        "        out, h_n = self.gru(emb)\n",
        "        # TODO : 마지막 hidden\n",
        "        last_h = h_n[-1]\n",
        "        logit = self.fc(last_h)\n",
        "        return logit, out, h_n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogOE47LuGP7i"
      },
      "source": [
        "학습 루프 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ELMItPqMGRG2"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=6, lr=1e-3, device=\"cpu\", tag=\"\"):\n",
        "    model = model.to(device)\n",
        "    # TODO : loss 함수 선언 (BCEWithLogitsLoss)\n",
        "    crit = nn.BCEWithLogitsLoss()\n",
        "    # TODO : optimizer 선언 (Adam)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # TODO : gradient 초기화\n",
        "            opt.zero_grad()\n",
        "            out = model(x)\n",
        "            logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "            # TODO : loss 계산\n",
        "            loss = crit(logit, y)\n",
        "            # TODO : backprop\n",
        "            loss.backward()\n",
        "            # TODO : optimizer step\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            n += x.size(0)\n",
        "\n",
        "        train_loss = total_loss / n\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                out = model(x)\n",
        "                logit = out[0] if isinstance(out, (tuple, list)) else out\n",
        "\n",
        "                # TODO : prob = sigmoid(logit)\n",
        "                prob = torch.sigmoid(logit)\n",
        "                # TODO : pred = (prob >= 0.5)\n",
        "                pred = (prob >= 0.5).float()\n",
        "\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.numel()\n",
        "\n",
        "        val_acc = correct / total\n",
        "        print(f\"{tag}[Epoch {ep}] train_loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MqgsERicGRFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421c5704-68df-4f8f-915c-7739c419f8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LSTM ===\n",
            "LSTM [Epoch 1] train_loss=0.6936  val_acc=0.4980\n",
            "LSTM [Epoch 2] train_loss=0.6933  val_acc=0.4935\n",
            "LSTM [Epoch 3] train_loss=0.6934  val_acc=0.4935\n",
            "LSTM [Epoch 4] train_loss=0.6934  val_acc=0.4985\n",
            "LSTM [Epoch 5] train_loss=0.6932  val_acc=0.5075\n",
            "LSTM [Epoch 6] train_loss=0.6932  val_acc=0.4935\n",
            "\n",
            "=== GRU ===\n",
            "GRU  [Epoch 1] train_loss=0.6936  val_acc=0.4820\n",
            "GRU  [Epoch 2] train_loss=0.6938  val_acc=0.4935\n",
            "GRU  [Epoch 3] train_loss=0.6933  val_acc=0.5065\n",
            "GRU  [Epoch 4] train_loss=0.6936  val_acc=0.4935\n",
            "GRU  [Epoch 5] train_loss=0.6932  val_acc=0.4935\n",
            "GRU  [Epoch 6] train_loss=0.6932  val_acc=0.4980\n"
          ]
        }
      ],
      "source": [
        "# 그대로 실행하시면 됩니다.\n",
        "\n",
        "T = 300\n",
        "train_ds = LongMemoryDataset(n_samples=8000, T=T)\n",
        "val_ds   = LongMemoryDataset(n_samples=2000, T=T)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "\n",
        "lstm = LSTMClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "gru  = GRUClassifier(vocab_size=3, embed_dim=8, hidden_dim=32)\n",
        "\n",
        "print(\"=== LSTM ===\")\n",
        "lstm = train_model(lstm, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"LSTM \")\n",
        "\n",
        "print(\"\\n=== GRU ===\")\n",
        "gru = train_model(gru, train_loader, val_loader, epochs=6, lr=1e-3, device=device, tag=\"GRU  \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8o9PlFmGZMp"
      },
      "source": [
        "### Q1. LSTM과 GRU의 차이점에 대해서 간략하게 서술해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE8A2EFGeHV"
      },
      "source": [
        "Ans)\n",
        "\n",
        "LSTM과 GRU는 순차 데이터 처리를 위한 RNN 모델이지만, 구조 부분에서 차이가 있다.\n",
        "\n",
        "GRU는 LSTM보다 구조가 단순하고 연산량이 적어 학습 속도가 빠르다는 장점이 있으며, LSTM은 상대적으로 복잡하지만 장기 의존성이라는 문제를 보다 명시적으로 처리할 수 있다는 점이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRfrs3zGGiDa"
      },
      "source": [
        "### Q2. T=80에서 LSTM과 GRU의 학습 곡선을 비교하고, 어느 쪽이 더 안정적으로 수렴했는지 서술해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjAVyZcEGiBq"
      },
      "source": [
        "Ans)\n",
        "\n",
        "T=80에서 LSTM과 GRU 모두 train loss와 validation accuracy가 약 0.5 수준에서 큰 변화 없이 유지되어 뚜렷한 수렴은 보이지 않았다.\n",
        "\n",
        "하지만, LSTM은 GRU에 비해 validation accuracy의 epoch 간 변동 폭이 작아 상대적으로 더 안정적으로 수렴한 것으로 판단된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWhHz27cGiAD"
      },
      "source": [
        "### Q3. T를 80 → 150 → 300 순으로 늘려서 각각 실행해보고, 어떤 모델이 성능을 더 잘 유지하는지, 왜 그런 것 같은지를 서술해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpLgdVqOGh66"
      },
      "source": [
        "Ans)\n",
        "\n",
        "T의 값이 증가할수록 LSTM과 GRU 모두 성능이 약 0.5 수준으로 유지되어 장기 의존성 학습에 어려움을 보였다.\n",
        "\n",
        "이때, LSTM은 GRU보다 성능 변동이 적어 상대적으로 성능을 더 잘 유지하였다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}